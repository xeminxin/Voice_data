{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4305a2",
   "metadata": {},
   "source": [
    "# 소프트맥스 회귀\n",
    "- 선형 회귀의 한계점\n",
    "> - 비선형 데이터의 경우 적절한 예측을 할 수 없다.\n",
    "> - 이상치 데이터에 대해서 취약하다.\n",
    "- 로지스틱 회귀의 한계점\n",
    "> - 클래스 간 경계선이 비선형인 경우 분류 성능이 저하됨\n",
    "> - 클래스 간 데이터가 크게 불균형한 경우 분류 성능이 저하됨\n",
    "\n",
    "Softmax regression은 로지스틱 회귀의 일반화된 형태로, 다중 클래스 분류 문제에서 사용되는 알고리즘이다. 목적은 각 클래스에 대한 확률 출력.<br>\n",
    "소프트맥스 회귀는 입력 변수(x)와 출력 변수(y) 사이의 선형 관계를 모델링하며, 선형 회귀와 유사한 방법을 사용한다. 다만, 선형 회귀는 예측값을 연속적인 값으로 출력하는 반면, 소프트맥스 회귀는 각 클래스에 대한 확률 갑슬 출력한다.<br>\n",
    "모델은 입력변수(x)와 가중치(w)의 곱의 합에 편향(b)을 더한 값에 대해 소프트맥스 함수를 적용한다. 소프트맥스 함수는 각 클래스에 대한 확률 값을 출력하기 위해 모델의 출력 값을 0과 1 사이의 값으로 변환한다. 이렇게 변환된 값은 모든 클래스의 출력 값 합이 1이 되도록 정규화한다. 따라서, 각 클래스에 대한 확률 값은 소프트맥스 함수의 출력 값으로 계산된다.<br>\n",
    "softmax regression은 다중 클래스 분류 문제에서 효과적으로 사용됨\n",
    "\n",
    "- Softmax function의 정의\n",
    "> softmax(zj) = exp(z_j)/sum_{k=1}^{K}\\exp(z_k)\n",
    ">> z : 입력 벡터, j : 클래스 인덱스, 분모 : 입력 벡터의 모든 원소에 대해 지수 함수를 취한 값의 합, exp(zj) : 입력 벡터의 j번째 원소에 대한 지수 함수 값 의미\n",
    "> 소프트맥스 함수는 입력 벡터의 모든 원소를 대상으로 계산하며, 각 클래스의 확률값을 계산함\n",
    "> 소프트맥스 함수의 이점 중 하나는 각 클래스에 대한 확률값이 항상 0과 1 사이의 값으로 나오며, 이들의 총합은 1이 된다는 것이다.\n",
    "> 이는 다중 클래스 분류 문제에서 각 클래스에 대한 예측 확률을 쉽게 해석할 수 있도록 해준다. 또한, 소프트맥스 함수는 출력값이 확률 분포를 따르기 때문에, 분류 모델에서 정확도를 최적화하는 목적 함수로 사용된다.\n",
    "\n",
    "- 소프트맥스 함수의 비용 함수\n",
    "크로스 엔트로피 함수. 이 함수는 모델의 예측값과 실제값 사이의 차이를 측정하여 모델의 성능을 평가하는 데 사용된다. 따라서 예측값이 실제값과 일치하도록 학습하는 것을 목적으로 한다. 이 함수의 값이 작을수록 모델의 예측이 실제값과 가까워진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918dfe6b",
   "metadata": {},
   "source": [
    "### Pytorch를 사용하여 소프트맥스 회귀 모델을 학습하고 예측하는 코드\n",
    "iris 데이터셋을 사용하여 소프트맥스 회귀 모델을 학습하고 테스트 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350ec979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], step[10/12], Loss : 0.6910\n",
      "Epoch [2/100], step[10/12], Loss : 0.8725\n",
      "Epoch [3/100], step[10/12], Loss : 0.8317\n",
      "Epoch [4/100], step[10/12], Loss : 0.7932\n",
      "Epoch [5/100], step[10/12], Loss : 0.7771\n",
      "Epoch [6/100], step[10/12], Loss : 0.6729\n",
      "Epoch [7/100], step[10/12], Loss : 0.6727\n",
      "Epoch [8/100], step[10/12], Loss : 0.6346\n",
      "Epoch [9/100], step[10/12], Loss : 0.5910\n",
      "Epoch [10/100], step[10/12], Loss : 0.6798\n",
      "Epoch [11/100], step[10/12], Loss : 0.5145\n",
      "Epoch [12/100], step[10/12], Loss : 0.6254\n",
      "Epoch [13/100], step[10/12], Loss : 0.5856\n",
      "Epoch [14/100], step[10/12], Loss : 0.4794\n",
      "Epoch [15/100], step[10/12], Loss : 0.6746\n",
      "Epoch [16/100], step[10/12], Loss : 0.4515\n",
      "Epoch [17/100], step[10/12], Loss : 0.5169\n",
      "Epoch [18/100], step[10/12], Loss : 0.6148\n",
      "Epoch [19/100], step[10/12], Loss : 0.6684\n",
      "Epoch [20/100], step[10/12], Loss : 0.6556\n",
      "Epoch [21/100], step[10/12], Loss : 0.6111\n",
      "Epoch [22/100], step[10/12], Loss : 0.4140\n",
      "Epoch [23/100], step[10/12], Loss : 0.5731\n",
      "Epoch [24/100], step[10/12], Loss : 0.3342\n",
      "Epoch [25/100], step[10/12], Loss : 0.3241\n",
      "Epoch [26/100], step[10/12], Loss : 0.4425\n",
      "Epoch [27/100], step[10/12], Loss : 0.5691\n",
      "Epoch [28/100], step[10/12], Loss : 0.4745\n",
      "Epoch [29/100], step[10/12], Loss : 0.4420\n",
      "Epoch [30/100], step[10/12], Loss : 0.4874\n",
      "Epoch [31/100], step[10/12], Loss : 0.4062\n",
      "Epoch [32/100], step[10/12], Loss : 0.5048\n",
      "Epoch [33/100], step[10/12], Loss : 0.6674\n",
      "Epoch [34/100], step[10/12], Loss : 0.5270\n",
      "Epoch [35/100], step[10/12], Loss : 0.4788\n",
      "Epoch [36/100], step[10/12], Loss : 0.6716\n",
      "Epoch [37/100], step[10/12], Loss : 0.5258\n",
      "Epoch [38/100], step[10/12], Loss : 0.4071\n",
      "Epoch [39/100], step[10/12], Loss : 0.3975\n",
      "Epoch [40/100], step[10/12], Loss : 0.4139\n",
      "Epoch [41/100], step[10/12], Loss : 0.3205\n",
      "Epoch [42/100], step[10/12], Loss : 0.5156\n",
      "Epoch [43/100], step[10/12], Loss : 0.4304\n",
      "Epoch [44/100], step[10/12], Loss : 0.4326\n",
      "Epoch [45/100], step[10/12], Loss : 0.3436\n",
      "Epoch [46/100], step[10/12], Loss : 0.4970\n",
      "Epoch [47/100], step[10/12], Loss : 0.5115\n",
      "Epoch [48/100], step[10/12], Loss : 0.4270\n",
      "Epoch [49/100], step[10/12], Loss : 0.4136\n",
      "Epoch [50/100], step[10/12], Loss : 0.3788\n",
      "Epoch [51/100], step[10/12], Loss : 0.5841\n",
      "Epoch [52/100], step[10/12], Loss : 0.4356\n",
      "Epoch [53/100], step[10/12], Loss : 0.5340\n",
      "Epoch [54/100], step[10/12], Loss : 0.4634\n",
      "Epoch [55/100], step[10/12], Loss : 0.4412\n",
      "Epoch [56/100], step[10/12], Loss : 0.4120\n",
      "Epoch [57/100], step[10/12], Loss : 0.4198\n",
      "Epoch [58/100], step[10/12], Loss : 0.3916\n",
      "Epoch [59/100], step[10/12], Loss : 0.3082\n",
      "Epoch [60/100], step[10/12], Loss : 0.4041\n",
      "Epoch [61/100], step[10/12], Loss : 0.5005\n",
      "Epoch [62/100], step[10/12], Loss : 0.3312\n",
      "Epoch [63/100], step[10/12], Loss : 0.3489\n",
      "Epoch [64/100], step[10/12], Loss : 0.3932\n",
      "Epoch [65/100], step[10/12], Loss : 0.3208\n",
      "Epoch [66/100], step[10/12], Loss : 0.2701\n",
      "Epoch [67/100], step[10/12], Loss : 0.4532\n",
      "Epoch [68/100], step[10/12], Loss : 0.3978\n",
      "Epoch [69/100], step[10/12], Loss : 0.4161\n",
      "Epoch [70/100], step[10/12], Loss : 0.3172\n",
      "Epoch [71/100], step[10/12], Loss : 0.4780\n",
      "Epoch [72/100], step[10/12], Loss : 0.4080\n",
      "Epoch [73/100], step[10/12], Loss : 0.5066\n",
      "Epoch [74/100], step[10/12], Loss : 0.3303\n",
      "Epoch [75/100], step[10/12], Loss : 0.2531\n",
      "Epoch [76/100], step[10/12], Loss : 0.4990\n",
      "Epoch [77/100], step[10/12], Loss : 0.2385\n",
      "Epoch [78/100], step[10/12], Loss : 0.3394\n",
      "Epoch [79/100], step[10/12], Loss : 0.2784\n",
      "Epoch [80/100], step[10/12], Loss : 0.2920\n",
      "Epoch [81/100], step[10/12], Loss : 0.2881\n",
      "Epoch [82/100], step[10/12], Loss : 0.2816\n",
      "Epoch [83/100], step[10/12], Loss : 0.2493\n",
      "Epoch [84/100], step[10/12], Loss : 0.3331\n",
      "Epoch [85/100], step[10/12], Loss : 0.4013\n",
      "Epoch [86/100], step[10/12], Loss : 0.3221\n",
      "Epoch [87/100], step[10/12], Loss : 0.3121\n",
      "Epoch [88/100], step[10/12], Loss : 0.3246\n",
      "Epoch [89/100], step[10/12], Loss : 0.2626\n",
      "Epoch [90/100], step[10/12], Loss : 0.3190\n",
      "Epoch [91/100], step[10/12], Loss : 0.4582\n",
      "Epoch [92/100], step[10/12], Loss : 0.2465\n",
      "Epoch [93/100], step[10/12], Loss : 0.2626\n",
      "Epoch [94/100], step[10/12], Loss : 0.5130\n",
      "Epoch [95/100], step[10/12], Loss : 0.3072\n",
      "Epoch [96/100], step[10/12], Loss : 0.3880\n",
      "Epoch [97/100], step[10/12], Loss : 0.2432\n",
      "Epoch [98/100], step[10/12], Loss : 0.3150\n",
      "Epoch [99/100], step[10/12], Loss : 0.2661\n",
      "Epoch [100/100], step[10/12], Loss : 0.4354\n",
      "Test Accuracy : 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to Pytorch tensors\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "# Create a Pytorch DataLoader object for the training set\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the softmax regression model\n",
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Set the hyperparameters\n",
    "input_size = 4\n",
    "num_classes = 3\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Create the softmax regression model and optimizer\n",
    "model = SoftmaxRegression(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print the loss every 10 batches\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(\"Epoch [{}/{}], step[{}/{}], Loss : {:.4f}\" .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "# Evaluate the model on the testing set\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print('Test Accuracy : {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c00c23",
   "metadata": {},
   "source": [
    "# Softmax Regression 모델이 학습한 결정 경계와 데이터 포인트 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63a254a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Forward Pass amd calculate loss\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m---> 38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIpytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1150\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIpytorch\\lib\\site-packages\\torch\\nn\\functional.py:2846\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2845\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create toy dataset\n",
    "x, y = make_blobs(n_samples=100, centers=3, n_features=2, random_state=42)\n",
    "\n",
    "# Convert data to Pytorch tensors\n",
    "x = torch.from_numpy(x).float()\n",
    "y = torch.from_numpy(y)\n",
    "\n",
    "# Define the softmax regression model\n",
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Instantiate model\n",
    "input_size = 2\n",
    "num_classes = 3\n",
    "model = SoftmaxRegression(input_size, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward Pass amd calculate loss\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "        \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    # Print the loss every 10 batches\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(\"Epoch [{}/{}], Loss : {:.4f}\" .format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot decision boundary\n",
    "x_min, x_max = x[:, 0].min() - 0.5, x[:, 0].max() + 0.5\n",
    "y_min, y_max = x[:, 1].min() - 0.5, x[:, 1].max() + 0.5\n",
    "xx, yy = torch.meshgrid(torch.arange(x_min, x_max, 0.1), torch.arange(y_min, y_max, 0.1))\n",
    "z = model(torch.cat((xx.reshape(-1, 1), yy.reshape(-1, 1)), dim=1)).argmax(dim=1)\n",
    "z = z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, z, alpha=0.4)\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=20, edgecolors='k')\n",
    "plt.title('Softmax Regression')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17eba4",
   "metadata": {},
   "source": [
    "# 위 코드는 다시 체크 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a407e",
   "metadata": {},
   "source": [
    "## 소프트맥스 회귀의 한계\n",
    "- 과적합 : 매개변수의 수가 많은 모델은 데이터를 잘 설명할 수 있지만, 일반화 능력이 떨어지는 과적합 문제가 발생할 수 있다.\n",
    "- 클래스 수에 따른 모델 복잡도 증가 : 클래스 수가 증가할수록 모델의 복잡도가 증가하여, 학습 데이터가 적을 때 모델의 성능이 저하될 수 있다.\n",
    "- 이진 분류만 가능 : 소프트맥스 회귀는 이진 분류만 가능하며, 다중 분류에 대한 처리를 위해서는 One-vs-Rest 또는 One-vs-One 방법을 사용해야 한다.\n",
    "## 소프트맥스 회귀의 한계 극복 방법\n",
    "- 정규화 : L1 또는 L2 정규화 등의 방법 사용\n",
    "- 드롭 아웃 : 일부 매개변수를 무작위로 제거해 모델의 일반화 능력 향상\n",
    "- 다중 레이블 분류\n",
    "- 다른 분류 모델과 결합\n",
    "- 다른 확률 분포 모델과 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb93eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIpytorch",
   "language": "python",
   "name": "aipytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
