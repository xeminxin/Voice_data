{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f672b5",
   "metadata": {},
   "source": [
    "# 옵티마이저 소개 및 종류\n",
    "옵티마이저란, 딥러닝 모델의 학습 과정에서 모델의 파라미터를 업데이트하는 알고리즘이다. 즉, 모델이 학습 데이터에 대해 최적의 결과를 도출하기 위해 모델의 weight와 bias를 조정하는 데 사용된다.<br>\n",
    "옵티마이저는 loss function에서 계산된 gradient를 이용해 모델의 파라미터를 업데이트 한다. loss function은 모델의 예측값과 실제값의 차이를 계산하는 함수로, 이 값을 최소화하는 방향으로 모델의 parameter를 조정한다.\n",
    "\n",
    "### 옵티마이저 종류\n",
    "- 경사하강법(Gradient Descent)\n",
    "> 가장 기본적인 최적화 알고리즘 중 하나. 가중치를 조정할 때 매개변수에 대한 손실함수의 기울기를 이용해 최적화 하는 방법. 즉, 가중치 업데이트는 현재 가중치에서 기울기를 빼는 방식으로 이루어짐<br>\n",
    ">> - Batch Gradient Descent : 전체 데이터셋에 대해 기울기를 계산하고 가중치 업데이트 -> 속도 느릴 수 있음, 메모리 사용량 많음, 하지만 최소값에 수렴하는 속도는 빠름<br>\n",
    ">> - Gradient Descent Optimization : Stochastic Gradient Descent(SGD)는 데이터 하나씩 기울기를 계산하고 가중치 업데이트 -> 메모리 사용량 적음, 하지만 최소값에 수렴하는 속도는 느릴 수 있음, 경사하강이 매번 달라져서 최적값에 수렴하는 과정에서 지그재그로 이동하는 현상 발생할 수 있음\n",
    ">> - Mini-batch Gradient Descent : 전체 데이터셋의 일부에 대해서만 기울기를 계산하고 가중치 업데이트 -> 메모리 사용량 적음, 한 개의 데이터를 처리하는 SGD보다 안정적으로 최적값에 수렴할 수 있다.\n",
    "- 모멘텀(Momentum)\n",
    "> 이전 기울기의 방향과 크기를 고려하여 새로운 기울기 계산. 이전 기울기의 방향이 현재 기울기와 일치하면 가중치를 더 크게 업데이트하고, 그렇지 않으면 더 작게 업데이트 한다. 일반적인 Gradient Descent는 지그재그로 움직이게 되지만 모멘텀은 이전 방향을 유지하면서 최적점에 빠르게 수렴할 수 있다. 모멘텀 값이 0에 가까울수록 Gradient Descent와 유사해지며, 1에 가까울수록 이전 방향을 보존하는 정도가 높아진다. 적절한 모멘텀 값을 설정하면 보다 빠르고 안정적인 학습을 할 수 있다.<br>\n",
    ">> - Standard Mementum : 기본적인 모멘텀으로 이전 기울기의 방향과 크기를 고려하여 가중치 업데이트<br>\n",
    ">> Nesterov Accelerated Gradient(NAG) : Standard Momentum에 비해 더 빠른 수렴 속도를 가지는 방법이다. 현재 위치에서 모멘텀 방향으로 이동한 후, 이동한 위치에서 기울기를 계산하여 가중치 업데이트<br>\n",
    ">> Heavy-ball Momentum : Standard Momentum에서 모멘텀 방향의 속도를 감소시켜 overshooting(최적값을 지나쳐서 발산하는 현상)을 막는 방법이다. 이전 기울기와 현재 기울기의 합과 차를 고려하여 가중치 업데이트\n",
    "- 아다그라드(Adagrad)\n",
    "> 각각의 매개변수에 서로 다른 학습률을 적용하는 방식 사용. 데이터셋에서 매개변수에 대한 제곱된 gradient의 역사를 누적함으로써 각각의 매개변수에 대한 적응적은 학습률을 계산한다. 기울기 제곱 값의 누적 합을 이용해 학습률을 조절하는 방식으로, 매개변수별로 학습률을 조절할 수 있는 방법이다. 이전 기울기들의 제곱을 누적하여 학습률을 조절하므로 처음에는 크게 업데이트하다가 점차 학습률이 줄어들게 된다. 이러한 방식은 기울기의 크기가 큰 매개변수는 학습률이 감소하고, 기울기의 크기가 작은 매개변수는 학습률이 증가하는 경향을 보인다.<br>\n",
    ">> 개별매개변수의 학습률을 적절하게 조절할 수 있어 매개변수의 스케일에 덜 민감해진다는 점이 있다. 하지만, 누적된 제곱 기울기 값이 계속해서 커져서 학습률이 점점 작아지는 문제가 있어, 학습이 오래될수록 업데이트가 매우 느려지는 경향이 있음.<br>\n",
    "파라미터는 하나만 필요\n",
    "- 알엠에스프롭(RMSprop)\n",
    "> 기울기 제곱의 이동평균을 사용하여 학습률 조절. 각 매개변수에 대한 학습률을 따로 설정한다. 이 때 빈번하게 발생하는 기울기의 작은 값으로 인해 학습률이 지나치게 작아지는 문제가 발생한다. RMSprop은 이 문제를 해결하기 위해 기울기 제곱의 이동평균을 사용한다. 이동평균을 구할 때 지수이동평균(EMA)을 사용한다. EMA는 이동평균을 구할 때 과거의 값들을 지수적으로 감소시키면서 현재값을 계산하는 방식이다. 이 때 이동편균을 구하는 지수 가중치를 하이퍼파라미터로 설정할 수 있다.<br>\n",
    ">> RMSprop의 학습률 조절 방법 : 기울기 제곱의 이동평균을 구한다. -> 구한 이동평균으로 학습률을 조절한다. 즉, 과거 기울기의 크기를 고려하면서 적절한 학습률을 계산한다.\n",
    "- 아담(Adam)\n",
    "> Momentum과 Adagrad의 아이디어를 결합한 옵티마이저. 각 매개변수마다 적응적인 학습률을 사용하며, 이전 기울기의 지수 가중 이동 평균과 이전 기울기 제곱의 지수가중 이동 평균을 계산하여 학습률을 조정한다.<br>\n",
    ">> Adam 식<br>\n",
    ">> 1. 현재 시점의 기울기 계산\n",
    ">> 2. 이전 기울기의 지수 가중 이동 평균과 이전 기울기 제곱의 지수 가중 이동 평균 계산\n",
    ">> 3. 학습률을 적응적으로 조정\n",
    ">> 4. 매개변수 업데이트\n",
    "> 이전 기울기의 방향과 크기를 고려하면서 매개변수를 업데이트 하기 때문에, 경사면이 급격하게 변하는 지점에서 빠르게 최적점에 다가갈 수 있다. 또한, Adagrad처럼 각 매개변수마다 적응적인 학습률을 사용하기 때문에, 학습이 더욱 안정적으로 이루어질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045065ed",
   "metadata": {},
   "source": [
    "## 선형 회귀 모델의 학습에서 다양한 옵티마이저를 적용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e854c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54059c3",
   "metadata": {},
   "source": [
    "### Boston data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b376d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train >>  455\n",
      "x_test >>  51\n",
      "y_train >>  455\n",
      "y_test >>  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smjin\\anaconda3\\envs\\AIpytorch\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston_dataset = load_boston()\n",
    "x_data = boston_dataset.data    # 학습 데이터\n",
    "y_data = boston_dataset.target  # 라벨 데이터\n",
    "\n",
    "# 데이터 스케일\n",
    "scaler = StandardScaler()\n",
    "x_data = scaler.fit_transform(x_data)\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.1, random_state=42)\n",
    "print(\"x_train >> \", len(x_train))\n",
    "print(\"x_test >> \", len(x_test))\n",
    "print(\"y_train >> \", len(y_train))\n",
    "print(\"y_test >> \", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab76ddf4",
   "metadata": {},
   "source": [
    "### 모델 생성 및 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c700ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_data.shape[1]  # 13\n",
    "output_dim = 1\n",
    "lr = 0.0001\n",
    "num_epochs = 1000\n",
    "\n",
    "model = nn.Linear(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478d9f4",
   "metadata": {},
   "source": [
    "### 다양한 옵티마이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3d0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\"SGD\" : optim.SGD(model.parameters(), lr=lr),\n",
    "            \"Momentum\" : optim.SGD(model.parameters(), lr=lr, momentum=0.9),\n",
    "            \"Adagrad\" : optim.Adagrad(model.parameters(), lr=lr),\n",
    "            \"RMSprop\" : optim.RMSprop(model.parameters(), lr=lr),\n",
    "            \"Adam\" : optim.Adam(model.parameters(), lr=lr)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6b033",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05bfe386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD - Epoch [100/1000], Loss: 100.4588\n",
      "SGD - Epoch [200/1000], Loss: 539.5785\n",
      "SGD - Epoch [300/1000], Loss: 195.6327\n",
      "SGD - Epoch [400/1000], Loss: 410.5547\n",
      "SGD - Epoch [500/1000], Loss: 345.2058\n",
      "SGD - Epoch [600/1000], Loss: 255.8973\n",
      "SGD - Epoch [700/1000], Loss: 489.2890\n",
      "SGD - Epoch [800/1000], Loss: 136.6813\n",
      "SGD - Epoch [900/1000], Loss: 571.8954\n",
      "SGD - Epoch [1000/1000], Loss: 98.5846\n",
      "Momentum - Epoch [100/1000], Loss: 97.6165\n",
      "Momentum - Epoch [200/1000], Loss: 234.0833\n",
      "Momentum - Epoch [300/1000], Loss: 957.0825\n",
      "Momentum - Epoch [400/1000], Loss: 501456.9688\n",
      "Momentum - Epoch [500/1000], Loss: 116150576.0000\n",
      "Momentum - Epoch [600/1000], Loss: 13191459840.0000\n",
      "Momentum - Epoch [700/1000], Loss: 535798153216.0000\n",
      "Momentum - Epoch [800/1000], Loss: 5347576119296.0000\n",
      "Momentum - Epoch [900/1000], Loss: 25009373373792256.0000\n",
      "Momentum - Epoch [1000/1000], Loss: 8414575681509261312.0000\n",
      "Adagrad - Epoch [100/1000], Loss: 8314683950614183936.0000\n",
      "Adagrad - Epoch [200/1000], Loss: 8314683950614183936.0000\n",
      "Adagrad - Epoch [300/1000], Loss: 8314683950614183936.0000\n",
      "Adagrad - Epoch [400/1000], Loss: 8314683950614183936.0000\n",
      "Adagrad - Epoch [500/1000], Loss: 8314683950614183936.0000\n",
      "Adagrad - Epoch [600/1000], Loss: 8314683950614183936.0000\n",
      "Adagrad - Epoch [700/1000], Loss: 8314683950614183936.0000\n",
      "Adagrad - Epoch [800/1000], Loss: 8314683950614183936.0000\n",
      "Adagrad - Epoch [900/1000], Loss: 8314683950614183936.0000\n",
      "Adagrad - Epoch [1000/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [100/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [200/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [300/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [400/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [500/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [600/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [700/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [800/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [900/1000], Loss: 8314683950614183936.0000\n",
      "RMSprop - Epoch [1000/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [100/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [200/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [300/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [400/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [500/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [600/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [700/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [800/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [900/1000], Loss: 8314683950614183936.0000\n",
      "Adam - Epoch [1000/1000], Loss: 8314683950614183936.0000\n"
     ]
    }
   ],
   "source": [
    "for optimizer_name, optimizer in optimizers.items():\n",
    "#     print(optimizer_name, optimizer)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = torch.tensor(x_train, dtype=torch.float32)\n",
    "        labels = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Baxkward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"{optimizer_name} - Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ce193",
   "metadata": {},
   "source": [
    "Loss : inf는 모델의 학습 과정에서 발생하는 오류 중 하나이다. inf는 무한대를 나타내며, 손실 함수 값이 무한대로 발산하여 발생하는 문제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6d7af",
   "metadata": {},
   "source": [
    "## 옵티마이저의 특징과 장단점\n",
    "\n",
    "1. SGD\n",
    "> - 가장 기본적인 옵티마이저\n",
    "> - 무작위로 선택한 샘플(미니배치)의 그래디언트를 이용해 파라미터를 갱신\n",
    "> - 속도가 빠르고 구현이 간단하지만, 수렴속도가 느리고 지역 최소값에 빠질 가능성이 있다.\n",
    "2. Momentum\n",
    "> - SGD와 비슷한 방식으로 가중치 갱신을 수행하지만, 그래디언트의 지수 이동 평균을 계산해 진동을 줄임\n",
    "> - SGD에 비해 빠르게 수렴, 지역 최소값을 탈출하기 쉬움\n",
    "> - 하지만 모멘텀 값에 따라 최적값을 지나쳐서 수렴하는 overshooting 문제가 발생할 수 있음<br>\n",
    "- Adagrad(Adaptive Gradient)\n",
    "> - 학습이 진행됨에 따라 그래디언트의 크기에 따라 학습률을 조정해 가중치를 갱신하는 방법\n",
    "> - 각 파라미터마다 개별적으로 학습률을 조정하기 때문에 수렴속도가 빠르고, 하이퍼파라미터의 튜닝이 필요없다.\n",
    "> - 그러나, 학습이 진행됨에 따라 학습률이 감소해 더 이상 업데이트가 일어나지 않을 수 있다.<br>\n",
    "- RMSprop(Root Mean Square Propagation)\n",
    "> - Adagrad의 단점을 보완한 방법\n",
    "> - 학습률이 점점 줄어들도록 하는 대신, 지수 이동 평균을 사용하여 학습률을 유지함\n",
    "> - 적응형 학습률 방법 중 하나로 SGD보다 빠르게 수렴하며, 불필요한 파라미터의 업데이트를 줄여준다.\n",
    "- Adam\n",
    "> - RMSprop와 Momentum을 합친 것으로, 학습 속도가 빠르며 안정적인 수렴을 보장한다.\n",
    "> - 미분 값이 0인 지점에서도 움직이도록 하기 위한 편향 보정을 수행한다.\n",
    "> - 자동으로 학습률을 조정하며, 처음에는 큰 학습률로 시작하다가 이후에는 학습률을 점진적으로 감소시키므로 수렴 속도를 향상시킬 수 있다.\n",
    "> Gradient Descent에서 보여지는 최적점에서의 오실레이션 문제를 해결할 수 있다.\n",
    "> - 적응적인 learning rate를 제공하므로 빠르게 수렴할 수 있다.\n",
    "> - hyperparameter에 대한 민감도가 낮아서 하이퍼 파라미터를 튜닝하기 쉬운 편이다.\n",
    "> - 하지만 일부 문제에서 다른 옵티마이저보다 성능이 떨어지는 경우가 있다. 또한, 일부 하이퍼 파라미터를 튜닝하지 않으면 성능이 떨어질 수 있다.\n",
    "> - 더욱 복잡한 문제에 대해서는 다른 옵티마이저와 비교했을 때 성능이 떨어질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd603f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIpytorch",
   "language": "python",
   "name": "aipytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
