{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b370200",
   "metadata": {},
   "source": [
    "# 다중 선형\n",
    "경사 하강법을 이용한 다중 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80de2c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0 :  0.8907546215358821\n",
      "beta_1 :  2.0237699271134932\n",
      "beta_2 :  0.057888309859129156\n",
      "Predicted y value 2.9145245486493754\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 예시 데이터 생성\n",
    "x1 = np.array([1,2,3,4,5])\n",
    "x2 = np.array([0,1,0,1,0])\n",
    "y = np.array([3,5,7,9,11])\n",
    "\n",
    "# 선형 회귀에서 설명한 단순 선형 회귀코드와 유사 \n",
    "# 다만, 입력 변수가 2개인 다중 선형 회귀에서는 입력 변수의 정규화가 필요하다. \n",
    "\n",
    "def gradient_descent(x1, x2, y, alpha, iterations):\n",
    "    # 초기값 설정\n",
    "    n = len(y)\n",
    "    beta_0 = 0\n",
    "    beta_1 = 0\n",
    "    beta_2 = 0\n",
    "    \n",
    "    # 경사하강법 수행\n",
    "    for i in range(iterations):\n",
    "        y_pred = beta_0 + beta_1 * x1 + beta_2 * x2\n",
    "        error = y_pred - y\n",
    "        beta_0 -= alpha * (1/n) * np.sum(error)\n",
    "        beta_1 -= alpha * (1/n) * np.sum(error * x1)\n",
    "        beta_2 -= alpha * (1/n) * np.sum(error * x2)\n",
    "        \n",
    "    return beta_0, beta_1, beta_2\n",
    "# 결과적으로 beta_0, beta_1, beta_2를 반환하고 출력한다.\n",
    "\n",
    "beta_0, beta_1, beta_2 = gradient_descent(x1, x2, y, 0.01, 1000)\n",
    "print(\"beta_0 : \", beta_0)\n",
    "print(\"beta_1 : \", beta_1)\n",
    "print(\"beta_2 : \", beta_2)\n",
    "\n",
    "x1_new = 1\n",
    "x2_new = 0\n",
    "\n",
    "y_pred = beta_0 + beta_1 * x1_new + beta_2 * x2_new\n",
    "\n",
    "print(\"Predicted y value\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4efb3a",
   "metadata": {},
   "source": [
    "위 코드는 다중 선형 회귀에서 경사하강법을 수행하여 회귀 계수 beta_0, beta_1, beta_2를 구하는 함수이다.<br>\n",
    "iterations : 경사 하강법을 몇 번 반복할지를 정하는 파라미터<br>\n",
    "alpha : 학습률(learning rate). 경사 하강법에서 한 번에 얼마나 많이 이동할지를 조절함. 이 값이 너무 작으면 학습이 느리고, 너무 크면 발산할 가능성이 있다.<br>\n",
    "y_pred : 현재 beta_0, beta_1, beta_2 값으로부터 예측한 종속 변수 y의 값.<br> error : 예측값과 실제값의 차이. 이를 이용해 각 회귀 계수에 대해 경사 하강법을 수행한다.<br>\n",
    "beta_0 -> 단순 선형 회귀에서와 같이 오차의 평균값을 빼준다.<br>\n",
    "beta_1과 beta_2는 오차와 독립 변수 x_1 및 x_2의 곱의 평균값을 빼준다.<br>\n",
    "마지막으로 계산된 beta_0, beta_1, beta_2를 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d265761",
   "metadata": {},
   "source": [
    "## 경사 하강법의 성능 평가와 개선 방법\n",
    "- 경사하강법에서 발생할 수 있는 문제점\n",
    "\n",
    "> 경사하강법이 수렴하지 않는 문제\n",
    ">> 원인 : 학습률이 너무 크거나 작거나, 목적 함수가 비선형 함수인 경우<br>\n",
    "해결 방법 : 학습률을 조절하거나, 더 나은 초기값을 선택하거나, 다른 알고리즘을 사용<br>\n",
    "\n",
    "> 지역 최소값에 수렴하는 문제\n",
    ">> 원인 : 목적 함수가 복잡하고 다중 국소 최소값이 존재하는 경우<br>\n",
    "해결 방법 : 초기값을 다르게 설정하거나, 모멘텀이나 다른 최적화 알고리즘을 사용<br>\n",
    "\n",
    "> 과적합 문제\n",
    ">> 원인 : 학습 데이터에 너무 맞춰져서 일반화 성능이 떨어지는 경우<br>\n",
    ">> 해결 방법 : 규제 기법을 사용해 모델 복잡도 제한 / 데이터를 더 많이 수집해 다양한 상황에 대응 / 특성 선택 등의 방법으로 모델의 복잡도 줄이기<br>\n",
    "\n",
    "> 수치적 불안정성 문제\n",
    ">> 원인 : 경사 하강법에서 계산되는 수치값이 커지면 수치적 불안정성 문제 발생<br>\n",
    ">> 해결방법 : 정규화를 사용하여 수치값의 범위를 조절하거나, 경사 하강법 대신 다른 최적화 알고리즘을 사용하는 등의 방법을 사용\n",
    "\n",
    "\n",
    "- 경사 하강법의 성능 평가와 개선 방법\n",
    "학습률과 반복 횟수에 따른 모델 성능 변화 평가 방법<br>\n",
    "가장 일반적인 검증 방법은 학습 및 검증 데이터셋으로 나누는 것이다. 이를 위해 무작위로 분할하여 일부는 학습 데이터로, 일부는 검증 데이터로 사용한다. 학습 데이터를 이용하여 모델을 학습시키고, 검증 데이터를 이용하여 모델의 성능을 평가한다.<br>\n",
    "모델의 성능을 평가하는 지표로는 MSE나 결정계수(R2) 등이 사용된다.\n",
    "\n",
    "- 그리드 탐색\n",
    "하이퍼파라미터를 조정하여 모델의 성능을 최적화하는 방법 중 하나.<br>\n",
    "하이퍼파라미터는 모델링 과정에서 사람이 직접 지정해줘야 하는 매개변수이다.<br>\n",
    "그리드 탐색은 가능한 하이퍼파라미터의 조합을 모두 만들어내고 이를 순차적으로 모델 학습을 진행하여 가장 좋은 성능을 보이는 조합을 선택하는 방법이다. 이 때 조합을 만드는 과정에서는 가능한 모든 조합을 만드는 것이 일반적이다.<br>\n",
    "그리드 탐색은 간단하고 직관적인 방법이지만, 하이퍼파라미터 공간이 클 경우에는 계산 비용이 크게 증가할 수 있으며 최적의 하이퍼파라미터를 찾지 못할 가능성도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd94b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIpytorch",
   "language": "python",
   "name": "aipytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
