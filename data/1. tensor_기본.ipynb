{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c43157",
   "metadata": {},
   "source": [
    "## 텐서 기본 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c779b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53b85b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troch 버전 >>  1.10.1\n",
      "torchvision version >>  0.11.2\n"
     ]
    }
   ],
   "source": [
    "print(\"troch 버전 >> \", torch.__version__)\n",
    "print(\"torchvision version >> \", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc32dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f3001",
   "metadata": {},
   "source": [
    "#### 텐서 초기화하기 데이터로부터 직접 텐서를 생성할 수 있다. 데이터의 자료형은 자동으로 유추\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f16750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 1. torch 이용해서 만든 텐서\n",
    "data = [[1,2], [3,4]]\n",
    "print(type(data))\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "\n",
    "# 2. Numpy -> torch tensor\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0c9f0",
   "metadata": {},
   "source": [
    "- torch.tensor()는 입력 텐서를 복사하여 새로운 텐서를 만든다. 이 함수는 항상 새로운 메모리를 할당하므로, 원본 데이터와의 메모리 공유가 이루어지지 않는다.\n",
    "- torch.from_numpy() 함수는 numpy 배열을 pytorch 텐서로 변환할 때, 원본 데이터와의 메모리 공유를 유지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f07bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones Tensor : \n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "Random Tensor : \n",
      "tensor([[0.9830, 0.9306],\n",
      "        [0.3015, 0.6716]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(f\"ones Tensor : \\n{x_ones}\")\n",
    "# torch.ones_like() 주어진 입력 텐서와 동일한 크기의 텐서를 생성하고 모든 요소를 1로 채우면 된다.\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)  # x_data 속성을 덮어쓴다.\n",
    "print(f\"Random Tensor : \\n{x_rand}\")\n",
    "# torch.rand_like() 주어진 입력 텐서와 동일한 크기의 텐서를 생성하고 모든 요소를 랜덤한 값으로 채운다. 그리고 타입 지정하면 그 타입으로 변경된다.\n",
    "# 0과 1 사이의 랜덤한 값으로 초기화 되고 데이터 타입 유형은 dtype=torch.float 지정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f554374c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_tensor \n",
      " tensor([[0.2138, 0.4071, 0.0836],\n",
      "        [0.0311, 0.1723, 0.7193]])\n",
      "ones_tensor \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "zeros_tensor \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[8.1709, 9.1610, 6.0789, 7.0330, 7.5559, 9.4307],\n",
      "        [7.9741, 9.3196, 6.5208, 6.7950, 7.5188, 7.8547],\n",
      "        [6.8034, 7.1655, 9.5533, 6.2348, 6.0133, 8.0222],\n",
      "        [6.4175, 6.4517, 8.9776, 9.9812, 6.3154, 8.2574],\n",
      "        [9.1376, 9.3100, 9.2110, 7.9392, 6.0611, 9.4936]])\n"
     ]
    }
   ],
   "source": [
    "# 무작위 또는 상수 값을 사용하기\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(\"rand_tensor \\n\", rand_tensor)\n",
    "print(\"ones_tensor \\n\", ones_tensor)\n",
    "print(\"zeros_tensor \\n\", zeros_tensor)\n",
    "\n",
    "# 유효 범위를 최소값 얼마부터 - 최대값 얼마까지\n",
    "shape_temp = (5, 6)\n",
    "min_val = 6\n",
    "max_val = 10\n",
    "rand_tensor_temp = torch.rand(shape_temp) * (max_val - min_val) + min_val\n",
    "print(rand_tensor_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00556901",
   "metadata": {},
   "source": [
    "shape = (2, 3, 4)는 3차원 텐서를 생성하는 것을 의미합니다. 이 텐서는 2개의 행렬로 구성되며, 각 행렬은 3행 4열의 형상을 갖습니다. 즉, (2, 3, 4)는 차원이 2개인 3행 4열의 텐서를 생성합니다.<br>\n",
    "반면에 shape = (2, 3,)는 2차원 텐서를 생성하는 것을 의미합니다. 이 텐서는 2개의 행벡터로 구성되며, 각 행벡터는 3개의 요소를 갖습니다. 즉, (2, 3,)는 2행 3열의 텐서를 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381295db",
   "metadata": {},
   "source": [
    "## 텐서 속성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17fe5542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3331, 0.8170, 0.6360, 0.4615],\n",
      "        [0.0339, 0.8309, 0.5833, 0.5368],\n",
      "        [0.7881, 0.0561, 0.8730, 0.8559]])\n",
      "cpu\n",
      "Shape of tensor : torch.Size([3, 4])\n",
      "Data type of tensor : torch.float32\n",
      "Device tensor is stored on : cpu\n"
     ]
    }
   ],
   "source": [
    "tensor_val = torch.rand(3, 4)\n",
    "print(tensor_val)\n",
    "\n",
    "# 디바이스 정보 가져오기\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# 외워두기\n",
    "\n",
    "# 디바이스를 변경하고자 하는 경우\n",
    "# 텐서의 디바이스를 변경하려면 to() 메서드를 사용할 수 있다. 이 메서드는 새로운 디바이스로 텐서를 이동시킨다.\n",
    "# Ex) model.to(device)\n",
    "\n",
    "print(f\"Shape of tensor : {tensor_val.shape}\")\n",
    "print(f\"Data type of tensor : {tensor_val.dtype}\")\n",
    "print(f\"Device tensor is stored on : {tensor_val.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7dcc145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.]])\n",
      "tensor([[1., 2., 1., 1.],\n",
      "        [1., 2., 1., 1.],\n",
      "        [1., 2., 1., 1.],\n",
      "        [1., 2., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 표준 인덱싱과 슬라이싱\n",
    "tensor_1 = torch.ones(4,4)\n",
    "tensor_1[:,3] = 0    # 특정값으로 바꾸기\n",
    "print(tensor_1)\n",
    "\n",
    "tensor_2 = torch.ones(4,4)\n",
    "tensor_2[:,1] = 2\n",
    "print(tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c18acfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 합치기\n",
    "# torch.cat 을 사용하여 주어진 차원에 따라 일련의 텐서를 연결할 수 있다.\n",
    "t1 = torch.cat([tensor_1, tensor_1, tensor_1], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04e1b9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.]])\n",
      "tensor([[1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "t_mult = tensor_1.mul(tensor_2)\n",
    "print(t_mult)\n",
    "\n",
    "print(tensor_1 * tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "674b0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.]])\n",
      "tensor([[7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.]])\n"
     ]
    }
   ],
   "source": [
    "# 행렬 곱\n",
    "print(tensor_2.matmul(tensor_2.T))\n",
    "print(tensor_2 @ tensor_2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1b57a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(t)\n",
    "n = t.numpy()\n",
    "print(n)\n",
    "\n",
    "t.add_(1)\n",
    "print(t)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610845c",
   "metadata": {},
   "source": [
    "## 뷰(View) - 원소의 수를 유지하면서 텐서의 크기 변경\n",
    "파이토치 텐서의 뷰는 넘파이에서 reshape와 같은 역할<br>\n",
    "Reshape -> 텐서의 크기를 변경해주는 역할\n",
    "reshape라는 이름에서 알 수 있듯이, 텐서의 크기(shape)를 변경해주는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95f3d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3차원 데이터 생성\n",
    "t_temp = np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t_temp)\n",
    "print(ft)\n",
    "print(ft.shape)\n",
    "\n",
    "# 이제 ft view를 이용해서 2차원 텐서로 변경\n",
    "print(ft.view([-1, 3]))   # = (?, 3) => 너가 알아서 해라.\n",
    "# 3의 크기로 바꿔라\n",
    "print(ft.view([-1, 3]).shape)\n",
    "# -1의 의미 : 나는 그 값을 모르겠음 파이토치 너가 알아서 해!!\n",
    "# 두 번째 차원은 길이는 3을 가지도록 하라는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b15975",
   "metadata": {},
   "source": [
    "view() 메서드를 사용하여 텐서의 차원을 변경하면 -> 데이터를 복사하여 새로운 텐서를 생성하고 이 새로운 텐서는 원래 텐서와 메모리를 공유하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d137afcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017428ad",
   "metadata": {},
   "source": [
    "## 스퀴즈\n",
    "1차원을 제거<br>\n",
    "스퀴즈는 차원이 1인 경우에는 해당 차언을 제거한다.<br>\n",
    "실습 3x1 크기를 가지는 2차원 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "786ed3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor(([0], [1], [2]))\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e1af694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef5d4a",
   "metadata": {},
   "source": [
    "## 언스퀴즈\n",
    "특정 위치에서 1인 차원을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "671cda87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft_temp = torch.Tensor([0, 1, 2])\n",
    "print(ft_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c264a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 차원에서 1차원 추가\n",
    "# 인덱스 0\n",
    "print(ft_temp.unsqueeze(0))\n",
    "print(ft_temp.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55df90d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft_temp.view(1, -1))\n",
    "print(ft_temp.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b7b45",
   "metadata": {},
   "source": [
    "## 사용자가 정의한 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee4469",
   "metadata": {},
   "source": [
    "- __init__() 함수는 이미지, csv 읽기, 변환 할당, 데이터 필터링 등과 같은 초기 논리가 발생하는 곳이다.\n",
    "- __getitem__() : 데이터와 레이블을 반환한다. 이 함수는 dataloader에서 호출된다.\n",
    "- __len__() : 보유한 샘플 수를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598b20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9b925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset) :\n",
    "    def __init__(self, image_paths, transform = None) :\n",
    "        # init() 메서드에서는 이미지 파일의 경로와 레이블, 그리고 이미지를 변환하기 위한 transform 인자를 받는다. 이 인자를 통해 데이터셋에 대한 다양한 전처리를 수행할 수 있다.\n",
    "        self.image_paths = glob.glob(os.path.join(image_paths, \"*\", \"*.jpg\"))\n",
    "        self.transform = transform\n",
    "        self.label_dict = {\"dew\" : 0, \"fogsmog\" : 1, \"frost\" : 2, \"glaze\" : 3, \"hail\" : 4, \"lightning\" : 5, \"rain\" : 6, \"rainbow\" : 7, \"rime\" : 8, \"sandstorm\" : 9, \"snow\" : 10}\n",
    "        \n",
    "    def __getitem__(self, index) :\n",
    "        # getitem() 메서드에서는 주어진 인덱스에 해당하는 이미지 파일을 읽어오고, 해당 이미지 파일의 레이블을 반환한다. 이 메서드에서는 transform 인자를 사용해 이미지를 변환한다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        # ./sample_data_01/lightning/2100.jpg\n",
    "        folder_name = image_path.split(\"/\")\n",
    "        folder_name = folder_name[2]\n",
    "        \n",
    "        label = self.label_dict[folder_name]\n",
    "        \n",
    "        # label\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self) :\n",
    "        # len() 메서드에서는 데이터셋의 크기를 반환한다. 이 경우에는 이미지 파일의 수를 반환한다.\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "# 데이터 경로 ./sample_data_01/이미지 폴더/image.jpg\n",
    "image_paths = \"./sample_data_01/\"   # 데이터 폴더 경로 지정\n",
    "dataset = CustomImageDataset(image_paths, transform=None)\n",
    "\n",
    "# 디버깅\n",
    "for i in dataset:\n",
    "    print(\"data and label\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a3558",
   "metadata": {},
   "source": [
    "## Pytorch DataLoader\n",
    "- Pytorch DataLoader는 데이터셋을 불러와서 모델에 입력으로 사용할 수 있는 형태로 변환해주는 유틸리티 클래스이다. DataLoader는 데이터셋을 불러오고, 데이터를 batch 단위로 분할하여 모델에 전달한다. 이를 통해 모델이 대용량의 데이터셋을 처리할 때 메모리 문제를 최소화할 수 있다.\n",
    "- DataLoader는 Pytorch에서 제공하는 torch.utils.data 패키지에 포함되어 있으며, 데이터셋과 함께 사용된다. DataLoader는 일반적으로 다음과 같은 인자를 받는다.\n",
    "- DataLoader는 각 배치를 반복하는 iterator 객체를 반환하며, 각 반복마다 모델에 전달할 입력 데이터와 해당 입력에 대한 레이블 데이터를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce5b6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch DataLoader 간단하게 소개\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "def is_grayscale(img):\n",
    "    return img.mode == 'L'\n",
    "# 이미지가 Grayscale인지 판단하는 함수\n",
    "\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset) :\n",
    "    def __init__(self, image_paths, transform = None) :\n",
    "        self.image_paths = glob.glob(os.path.join(image_paths, \"*\", \"*.jpg\"))\n",
    "        self.transform = transform\n",
    "        self.label_dict = {\"dew\" : 0, \"fogsmog\" : 1, \"frost\" : 2, \"glaze\" : 3, \"hail\" : 4, \"lightning\" : 5, \"rain\" : 6, \"rainbow\" : 7, \"rime\" : 8, \"sandstorm\" : 9, \"snow\" : 10}\n",
    "        \n",
    "    def __getitem__(self, index) :\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # 흑백 이미지 체크 -> RGB 채널 크기가 달라서 오류 발생. 흑백채널은 1 RGB 3이다.\n",
    "        if not is_grayscale(image) :\n",
    "            # ./sample_data_01/lightning/2100.jpg\n",
    "            folder_name = image_path.split(\"/\")\n",
    "            folder_name = folder_name[2]\n",
    "            # 라벨 정의\n",
    "            label = self.label_dict[folder_name]\n",
    "\n",
    "            # transform -> image aug\n",
    "            if self.transform :\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, label\n",
    "        \n",
    "        else:\n",
    "            print(\"흑백 이미지 >> \", image_path)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 데이터 경로 ./sample_data_01/이미지 폴더/image.jpg\n",
    "image_paths = \"./sample_data_01/\"  # 데이터 폴더 경로 지정\n",
    "# torch 제공하는 transform 라이브러리 사용하여 image aug 정의\n",
    "dataset = CustomImageDataset(image_paths, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "610f90e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# DataLoader를 이용하여 데이터 불러오기\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loader :\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images, labels)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AIpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[25], line 30\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_grayscale(image) :\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# ./sample_data_01/lightning/2100.jpg\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     folder_name \u001b[38;5;241m=\u001b[39m image_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m     folder_name \u001b[38;5;241m=\u001b[39m \u001b[43mfolder_name\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# 라벨 정의\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_dict[folder_name]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Dataloader 정의\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# DataLoader를 이용하여 데이터 불러오기\n",
    "for images, labels in data_loader :\n",
    "    print(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7a56e",
   "metadata": {},
   "source": [
    "### Pytorch DataLoader 간단하게 소개 - csv 데이터 이용하여 만들어 보기\n",
    "- 데이터 소개 - 키와 몸무게 데이터 (키 단위 : 인치 / 몸무게 단위 : 파운드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "780b3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키는 인치 몸무게는 파운드\n",
    "class HeightWeightDataset(Dataset) :\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = []\n",
    "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "            next(f)   # 첫 번째 라인은 헤더이므로 제외\n",
    "            for line in f:\n",
    "                # print(line)\n",
    "                _, height, weight = line.strip().split(\",\")\n",
    "                height = float(height)\n",
    "                weight = float(weight)\n",
    "                convert_to_kg_data = round(self.convert_to_kg(weight), 2)\n",
    "                convert_to_cm_data = round(self.inch_to_cm(height), 1)\n",
    "                \n",
    "                # print(convert_to_kg_data, convert_to_cm_data)\n",
    "                \n",
    "                self.data.append([convert_to_cm_data, convert_to_kg_data])\n",
    "                \n",
    "    def __getitem__(self, index) :\n",
    "        data = torch.tensor(self.data[index], dtype=torch.float)\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # 파운드 -> Kg 변경하는 함수\n",
    "    def convert_to_kg(self, weight_lb):\n",
    "        return weight_lb * 0.453592\n",
    "    \n",
    "    # 인치 -> cm 변경 하는 함수\n",
    "    def inch_to_cm(self, inch):\n",
    "        return inch * 2.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfa9d67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[170.2000]]) tensor([[58.4300]])\n",
      "tensor([[176.3000]]) tensor([[58.1800]])\n",
      "tensor([[175.]]) tensor([[63.6100]])\n",
      "tensor([[172.3000]]) tensor([[64.0800]])\n",
      "tensor([[168.6000]]) tensor([[54.5700]])\n",
      "tensor([[174.8000]]) tensor([[60.1800]])\n",
      "tensor([[174.2000]]) tensor([[58.]])\n",
      "tensor([[170.7000]]) tensor([[49.4500]])\n",
      "tensor([[177.3000]]) tensor([[64.1800]])\n",
      "tensor([[173.6000]]) tensor([[60.8600]])\n",
      "tensor([[169.6000]]) tensor([[55.2900]])\n",
      "tensor([[187.5000]]) tensor([[63.1900]])\n",
      "tensor([[175.5000]]) tensor([[52.4900]])\n",
      "tensor([[178.3000]]) tensor([[59.7100]])\n",
      "tensor([[177.7000]]) tensor([[59.1500]])\n",
      "tensor([[177.8000]]) tensor([[55.3600]])\n",
      "tensor([[172.2000]]) tensor([[65.4500]])\n",
      "tensor([[175.3000]]) tensor([[62.3900]])\n",
      "tensor([[163.8000]]) tensor([[51.2100]])\n",
      "tensor([[181.7000]]) tensor([[61.9100]])\n",
      "tensor([[176.6000]]) tensor([[56.8800]])\n",
      "tensor([[173.4000]]) tensor([[52.6600]])\n",
      "tensor([[178.4000]]) tensor([[60.7700]])\n",
      "tensor([[167.1000]]) tensor([[54.3800]])\n",
      "tensor([[170.7000]]) tensor([[57.5800]])\n",
      "tensor([[166.3000]]) tensor([[60.3100]])\n",
      "tensor([[170.1000]]) tensor([[57.9000]])\n",
      "tensor([[175.]]) tensor([[52.3800]])\n",
      "tensor([[167.]]) tensor([[53.9300]])\n",
      "tensor([[169.1000]]) tensor([[54.0900]])\n",
      "tensor([[169.7000]]) tensor([[47.5500]])\n",
      "tensor([[176.4000]]) tensor([[55.6100]])\n",
      "tensor([[173.5000]]) tensor([[58.8600]])\n",
      "tensor([[172.6000]]) tensor([[48.4700]])\n",
      "tensor([[179.3000]]) tensor([[61.7900]])\n",
      "tensor([[162.9000]]) tensor([[48.1300]])\n",
      "tensor([[167.6000]]) tensor([[50.4700]])\n",
      "tensor([[172.]]) tensor([[55.3700]])\n",
      "tensor([[180.6000]]) tensor([[63.5000]])\n",
      "tensor([[175.8000]]) tensor([[58.5600]])\n",
      "tensor([[182.4000]]) tensor([[63.5500]])\n",
      "tensor([[170.5000]]) tensor([[55.5500]])\n",
      "tensor([[163.6000]]) tensor([[56.1500]])\n",
      "tensor([[174.2000]]) tensor([[60.7100]])\n",
      "tensor([[173.1000]]) tensor([[61.9400]])\n",
      "tensor([[171.2000]]) tensor([[66.3800]])\n",
      "tensor([[168.9000]]) tensor([[49.6700]])\n",
      "tensor([[177.8000]]) tensor([[61.9000]])\n",
      "tensor([[173.5000]]) tensor([[56.9800]])\n",
      "tensor([[178.3000]]) tensor([[65.7700]])\n",
      "tensor([[176.3000]]) tensor([[69.4100]])\n",
      "tensor([[172.8000]]) tensor([[56.0100]])\n",
      "tensor([[167.1000]]) tensor([[54.8100]])\n",
      "tensor([[172.5000]]) tensor([[50.9700]])\n",
      "tensor([[165.9000]]) tensor([[52.5800]])\n",
      "tensor([[168.4000]]) tensor([[57.3700]])\n",
      "tensor([[171.]]) tensor([[57.3000]])\n",
      "tensor([[168.4000]]) tensor([[56.6100]])\n",
      "tensor([[167.3000]]) tensor([[52.7200]])\n",
      "tensor([[169.6000]]) tensor([[54.7300]])\n",
      "tensor([[168.4000]]) tensor([[58.4900]])\n",
      "tensor([[165.9000]]) tensor([[58.2000]])\n",
      "tensor([[166.2000]]) tensor([[51.6600]])\n",
      "tensor([[173.3000]]) tensor([[58.2000]])\n",
      "tensor([[173.4000]]) tensor([[52.8300]])\n",
      "tensor([[171.2000]]) tensor([[62.8100]])\n",
      "tensor([[180.3000]]) tensor([[72.1000]])\n",
      "tensor([[168.9000]]) tensor([[57.8100]])\n",
      "tensor([[176.9000]]) tensor([[66.0400]])\n",
      "tensor([[168.2000]]) tensor([[64.0300]])\n",
      "tensor([[175.8000]]) tensor([[58.4000]])\n",
      "tensor([[169.9000]]) tensor([[55.8100]])\n",
      "tensor([[175.]]) tensor([[62.4900]])\n",
      "tensor([[175.5000]]) tensor([[61.2000]])\n",
      "tensor([[174.5000]]) tensor([[55.9300]])\n",
      "tensor([[168.9000]]) tensor([[54.8000]])\n",
      "tensor([[174.5000]]) tensor([[59.3100]])\n",
      "tensor([[178.]]) tensor([[60.5400]])\n",
      "tensor([[172.]]) tensor([[60.2100]])\n",
      "tensor([[166.3000]]) tensor([[52.5900]])\n",
      "tensor([[176.6000]]) tensor([[46.8600]])\n",
      "tensor([[177.1000]]) tensor([[52.4400]])\n",
      "tensor([[169.6000]]) tensor([[49.5200]])\n",
      "tensor([[175.6000]]) tensor([[63.2600]])\n",
      "tensor([[173.3000]]) tensor([[61.6700]])\n",
      "tensor([[166.1000]]) tensor([[56.0100]])\n",
      "tensor([[161.1000]]) tensor([[55.8400]])\n",
      "tensor([[167.5000]]) tensor([[50.7600]])\n",
      "tensor([[171.8000]]) tensor([[51.7700]])\n",
      "tensor([[176.9000]]) tensor([[63.3500]])\n",
      "tensor([[171.1000]]) tensor([[56.5800]])\n",
      "tensor([[162.7000]]) tensor([[48.4000]])\n",
      "tensor([[171.9000]]) tensor([[55.8100]])\n",
      "tensor([[167.5000]]) tensor([[58.5800]])\n",
      "tensor([[173.3000]]) tensor([[64.5600]])\n",
      "tensor([[162.2000]]) tensor([[57.6900]])\n",
      "tensor([[171.4000]]) tensor([[59.6700]])\n",
      "tensor([[173.4000]]) tensor([[60.1400]])\n",
      "tensor([[178.5000]]) tensor([[56.9200]])\n",
      "tensor([[165.6000]]) tensor([[48.6100]])\n",
      "tensor([[161.2000]]) tensor([[44.4100]])\n",
      "tensor([[169.1000]]) tensor([[54.8500]])\n",
      "tensor([[175.1000]]) tensor([[61.3700]])\n",
      "tensor([[172.2000]]) tensor([[59.5800]])\n",
      "tensor([[171.4000]]) tensor([[49.3500]])\n",
      "tensor([[174.8000]]) tensor([[62.1000]])\n",
      "tensor([[168.4000]]) tensor([[54.4400]])\n",
      "tensor([[182.9000]]) tensor([[62.9500]])\n",
      "tensor([[165.6000]]) tensor([[56.6400]])\n",
      "tensor([[170.5000]]) tensor([[56.2600]])\n",
      "tensor([[177.6000]]) tensor([[63.6800]])\n",
      "tensor([[174.4000]]) tensor([[64.8500]])\n",
      "tensor([[166.4000]]) tensor([[50.3300]])\n",
      "tensor([[174.3000]]) tensor([[58.3100]])\n",
      "tensor([[171.3000]]) tensor([[57.5200]])\n",
      "tensor([[167.6000]]) tensor([[55.1100]])\n",
      "tensor([[184.]]) tensor([[62.0200]])\n",
      "tensor([[164.8000]]) tensor([[46.3100]])\n",
      "tensor([[168.8000]]) tensor([[58.7400]])\n",
      "tensor([[168.9000]]) tensor([[58.3900]])\n",
      "tensor([[177.9000]]) tensor([[57.8800]])\n",
      "tensor([[171.8000]]) tensor([[64.3400]])\n",
      "tensor([[173.8000]]) tensor([[58.7400]])\n",
      "tensor([[180.9000]]) tensor([[62.5500]])\n",
      "tensor([[175.]]) tensor([[65.1100]])\n",
      "tensor([[165.6000]]) tensor([[51.4400]])\n",
      "tensor([[170.5000]]) tensor([[61.2400]])\n",
      "tensor([[178.3000]]) tensor([[67.0800]])\n",
      "tensor([[179.9000]]) tensor([[60.7900]])\n",
      "tensor([[172.1000]]) tensor([[56.5400]])\n",
      "tensor([[167.6000]]) tensor([[48.2800]])\n",
      "tensor([[174.2000]]) tensor([[56.4100]])\n",
      "tensor([[180.6000]]) tensor([[58.1200]])\n",
      "tensor([[173.7000]]) tensor([[52.2700]])\n",
      "tensor([[171.9000]]) tensor([[54.9900]])\n",
      "tensor([[170.5000]]) tensor([[57.7200]])\n",
      "tensor([[174.4000]]) tensor([[58.4000]])\n",
      "tensor([[171.3000]]) tensor([[60.2600]])\n",
      "tensor([[178.3000]]) tensor([[54.9400]])\n",
      "tensor([[181.3000]]) tensor([[58.0100]])\n",
      "tensor([[179.9000]]) tensor([[61.3800]])\n",
      "tensor([[165.8000]]) tensor([[58.6500]])\n",
      "tensor([[171.6000]]) tensor([[60.7000]])\n",
      "tensor([[177.9000]]) tensor([[70.4800]])\n",
      "tensor([[173.6000]]) tensor([[57.8700]])\n",
      "tensor([[170.4000]]) tensor([[66.3600]])\n",
      "tensor([[180.9000]]) tensor([[59.2800]])\n",
      "tensor([[172.1000]]) tensor([[51.9600]])\n",
      "tensor([[173.6000]]) tensor([[62.8700]])\n",
      "tensor([[173.3000]]) tensor([[59.6500]])\n",
      "tensor([[175.5000]]) tensor([[52.4200]])\n",
      "tensor([[172.9000]]) tensor([[46.0300]])\n",
      "tensor([[170.4000]]) tensor([[59.1300]])\n",
      "tensor([[169.]]) tensor([[58.0900]])\n",
      "tensor([[169.7000]]) tensor([[64.3200]])\n",
      "tensor([[178.]]) tensor([[59.6900]])\n",
      "tensor([[167.2000]]) tensor([[54.7700]])\n",
      "tensor([[171.1000]]) tensor([[63.9100]])\n",
      "tensor([[166.7000]]) tensor([[55.9200]])\n",
      "tensor([[187.7000]]) tensor([[68.6700]])\n",
      "tensor([[179.2000]]) tensor([[59.7800]])\n",
      "tensor([[181.6000]]) tensor([[63.7800]])\n",
      "tensor([[173.3000]]) tensor([[49.7200]])\n",
      "tensor([[172.6000]]) tensor([[62.3100]])\n",
      "tensor([[167.9000]]) tensor([[52.5200]])\n",
      "tensor([[174.3000]]) tensor([[61.0700]])\n",
      "tensor([[175.4000]]) tensor([[64.6200]])\n",
      "tensor([[173.4000]]) tensor([[58.3000]])\n",
      "tensor([[173.7000]]) tensor([[54.3900]])\n",
      "tensor([[163.6000]]) tensor([[49.8400]])\n",
      "tensor([[172.6000]]) tensor([[56.3400]])\n",
      "tensor([[165.8000]]) tensor([[54.2100]])\n",
      "tensor([[172.8000]]) tensor([[57.8200]])\n",
      "tensor([[169.6000]]) tensor([[58.1200]])\n",
      "tensor([[177.2000]]) tensor([[61.7200]])\n",
      "tensor([[172.3000]]) tensor([[57.0600]])\n",
      "tensor([[178.8000]]) tensor([[70.7100]])\n",
      "tensor([[179.9000]]) tensor([[64.6000]])\n",
      "tensor([[168.6000]]) tensor([[54.1500]])\n",
      "tensor([[172.]]) tensor([[52.9900]])\n",
      "tensor([[177.3000]]) tensor([[65.1400]])\n",
      "tensor([[170.7000]]) tensor([[58.8400]])\n",
      "tensor([[166.6000]]) tensor([[44.7400]])\n",
      "tensor([[169.]]) tensor([[49.1400]])\n",
      "tensor([[176.4000]]) tensor([[58.2500]])\n",
      "tensor([[169.9000]]) tensor([[66.0800]])\n",
      "tensor([[172.]]) tensor([[55.5700]])\n",
      "tensor([[176.2000]]) tensor([[63.0700]])\n",
      "tensor([[178.4000]]) tensor([[64.1800]])\n",
      "tensor([[174.6000]]) tensor([[56.3500]])\n",
      "tensor([[176.6000]]) tensor([[61.7800]])\n",
      "tensor([[172.7000]]) tensor([[56.8500]])\n",
      "tensor([[175.8000]]) tensor([[50.8700]])\n",
      "tensor([[167.1000]]) tensor([[51.2500]])\n",
      "tensor([[163.3000]]) tensor([[46.6500]])\n",
      "tensor([[167.7000]]) tensor([[63.5700]])\n",
      "tensor([[177.6000]]) tensor([[66.6900]])\n",
      "tensor([[172.5000]]) tensor([[54.1200]])\n",
      "tensor([[167.4000]]) tensor([[52.9000]])\n",
      "tensor([[170.7000]]) tensor([[46.9600]])\n"
     ]
    }
   ],
   "source": [
    "dataset = HeightWeightDataset(\"./hw_200.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    \n",
    "    # batch[:, 0].unsqueeze(1) 2차원 텐서로 변화됩니다.\n",
    "    x = batch[:, 0].unsqueeze(1)\n",
    "    y = batch[:, 1].unsqueeze(1)\n",
    "    \n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa34a6e",
   "metadata": {},
   "source": [
    "## JSON 데이터 이용하여 만들어 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6070e71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'filename': 'image_001.jpg', 'width': 1280, 'height': 720, 'ann': {'bboxes': [[10, 10, 50, 50], [100, 100, 200, 200]], 'labels': [0, 1]}}, {'filename': 'image_002.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[20, 20, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_003.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[30, 30, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_004.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[10, 10, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}]\n",
      "[{'filename': 'image_001.jpg', 'width': 1280, 'height': 720, 'ann': {'bboxes': [[10, 10, 50, 50], [100, 100, 200, 200]], 'labels': [0, 1]}}, {'filename': 'image_002.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[20, 20, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_003.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[30, 30, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_004.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[10, 10, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}]\n",
      "[{'filename': 'image_001.jpg', 'width': 1280, 'height': 720, 'ann': {'bboxes': [[10, 10, 50, 50], [100, 100, 200, 200]], 'labels': [0, 1]}}, {'filename': 'image_002.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[20, 20, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_003.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[30, 30, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_004.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[10, 10, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}]\n",
      "[{'filename': 'image_001.jpg', 'width': 1280, 'height': 720, 'ann': {'bboxes': [[10, 10, 50, 50], [100, 100, 200, 200]], 'labels': [0, 1]}}, {'filename': 'image_002.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[20, 20, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_003.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[30, 30, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_004.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[10, 10, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}]\n",
      "[{'filename': 'image_001.jpg', 'width': 1280, 'height': 720, 'ann': {'bboxes': [[10, 10, 50, 50], [100, 100, 200, 200]], 'labels': [0, 1]}}, {'filename': 'image_002.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[20, 20, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_003.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[30, 30, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}, {'filename': 'image_004.jpg', 'width': 720, 'height': 1280, 'ann': {'bboxes': [[10, 10, 60, 60], [300, 300, 400, 400]], 'labels': [1, 2]}}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, json_path, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # 이미지 샘플이 없음 대신 이미지 경로 전달 (원래는 이미지 읽고 이미지를 보내야함)\n",
    "        # 간단하게 만드는법을 학습하기 위해서 이미지 샘플 제공 x\n",
    "        print(self.data)\n",
    "        img_path = self.data[index]['filename']\n",
    "        img_path = os.path.join(\"이미지 폴더\", img_path)\n",
    "        # 이미지 읽기\n",
    "        # img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 바운딩 박스 정보 읽기\n",
    "        bboxes = self.data[index]['ann']['bboxes']\n",
    "        labels = self.data[index]['ann']['labels']\n",
    "        \n",
    "        # 바운딩 박스 정보를 Tensor로 변환\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype = torch.float32)\n",
    "        \n",
    "        # 이미지 변환 : 단 주의사항 : 파이토치 제공하는 transforms 박스 위치에 따른 좌표 이동 x\n",
    "#         if self.transforms is not None:\n",
    "#             img = self.transforms(img)\n",
    "        \n",
    "        return img_path, {'boxes' : bboxes, 'labels' : labels}\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.data)\n",
    "    \n",
    "data = CustomDataset(\"./test.json\", transforms=None)\n",
    "\n",
    "for image_paths, anno in data:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae36c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIpytorch",
   "language": "python",
   "name": "aipytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
